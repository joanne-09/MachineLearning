{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "    # **Lab1: Regression**\n",
    "In *lab 1*, you need to finish:\n",
    "\n",
    "1.  Basic Part: Implement the regression model to predict people's grip force from their weight.\n",
    "You can use either Matrix Inversion or Gradient Descent.\n",
    "\n",
    "\n",
    "> *   Step 1: Split Data\n",
    "> *   Step 2: Preprocess Data\n",
    "> *   Step 3: Implement Regression\n",
    "> *   Step 4: Make Prediction\n",
    "> *   Step 5: Train Model and Generate Result\n",
    "\n",
    "2.  Advanced Part: Implementing a regression model to predict grip force in a different way (for example, with more variables) than the basic part\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "aMASY5gD9L0K"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# 1. Basic Part (50%)\n",
    "In the first part, you need to implement the regression to predict grip force\n",
    "\n",
    "Please save the prediction result in a CSV file and submit it to Kaggle"
   ],
   "metadata": {
    "id": "yNpd_FfX_BXI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Packages\n",
    "\n",
    "> Note: You **cannot** import any other package\n"
   ],
   "metadata": {
    "id": "egBMMLGV_X_x"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WhhUTua487C-",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.321651Z",
     "start_time": "2024-09-25T08:28:53.315895Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import random\n",
    "\n",
    "from debugpy.common.log import newline"
   ],
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Global attributes\n",
    "Define the global attributes\\\n",
    "You can also add your own global attributes here"
   ],
   "metadata": {
    "id": "8iuXHvhLALwz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_dataroot = 'lab1_basic_training.csv' # Training data file file named as 'lab1_basic_training.csv'\n",
    "testing_dataroot = 'lab1_basic_testing.csv'   # Testing data file named as 'lab1_basic_testing.csv'\n",
    "output_dataroot = 'lab1_basic.csv' # Output file will be named as 'lab1_basic.csv'\n",
    "coefficient_root = 'coefficient.csv'\n",
    "\n",
    "training_datalist =  [] # Training datalist, saved as numpy array\n",
    "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
    "\n",
    "output_datalist =  [] # Your prediction, should be a list with 100 elements"
   ],
   "metadata": {
    "id": "wXZVhdp8-flF",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.338886Z",
     "start_time": "2024-09-25T08:28:53.329970Z"
    }
   },
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the Input File\n",
    "First, load the basic input file **lab1_basic_training.csv** and **lab1_basic_testing.csv**\n",
    "\n",
    "Input data would be stored in *training_datalist* and *testing_datalist*"
   ],
   "metadata": {
    "id": "IyTqIRxQAtWj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Read input csv to datalist\n",
    "with open(training_dataroot, newline='') as csvfile:\n",
    "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
    "\n",
    "with open(testing_dataroot, newline='') as csvfile:\n",
    "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
   ],
   "metadata": {
    "id": "KUzYjoq9AwRp",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.373098Z",
     "start_time": "2024-09-25T08:28:53.356131Z"
    }
   },
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Implement the Regression Model\n",
    "\n",
    "> Note: It is recommended to use the functions we defined, you can also define your own functions"
   ],
   "metadata": {
    "id": "QFXG-axpAcom"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 1: Split Data\n",
    "Split data in *training_datalist* into training dataset and validation dataset\n"
   ],
   "metadata": {
    "id": "9bqYH_MvBv4v"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def SplitData(data, split_ratio):\n",
    "    \"\"\"\n",
    "    Splits the given dataset into training and validation sets based on the specified split ratio.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to be split. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "    - split_ratio (float): The ratio of the data to be used for training. For example, a value of 0.8 means 80% of the data will be used for training and the remaining 20% for validation.\n",
    "\n",
    "    Returns:\n",
    "    - training_data (numpy.ndarray): The portion of the dataset used for training.\n",
    "    - validation_data (numpy.ndarray): The portion of the dataset used for validation.\n",
    "\n",
    "    \"\"\"\n",
    "    training_data = []\n",
    "    validation_data = []\n",
    "\n",
    "    # TODO: i don't know probably done\n",
    "    idx = int(len(data) * split_ratio)\n",
    "    validation_data = data[idx:]\n",
    "    training_data = data[:idx]\n",
    "    \n",
    "    return training_data, validation_data\n",
    "\n"
   ],
   "metadata": {
    "id": "6K2QUnt-A-1r",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.399725Z",
     "start_time": "2024-09-25T08:28:53.393249Z"
    }
   },
   "outputs": [],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2: Preprocess Data\n",
    "Handle unreasonable data and missing data\n",
    "\n",
    "> Hint 1: Outliers and missing data can be addressed by either removing them or replacing them using statistical methods (e.g., the mean of all data).\n",
    "\n",
    "> Hint 2: Missing data are represented as `np.nan`, so functions like `np.isnan()` can be used to detect them.\n",
    "\n",
    "> Hint 3: Methods such as the Interquartile Range (IQR) can help detect outliers"
   ],
   "metadata": {
    "id": "-miSLyewCeME"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def PreprocessData(data):\n",
    "    \"\"\"\n",
    "    Preprocess the given dataset and return the result.\n",
    "\n",
    "    Parameters:\n",
    "    - data (numpy.ndarray): The dataset to preprocess. It is expected to be a 2D array where each row represents a data point and each column represents a feature.\n",
    "\n",
    "    Returns:\n",
    "    - preprocessedData (numpy.ndarray): Preprocessed data.\n",
    "    \"\"\"\n",
    "    preprocessedData = []\n",
    "\n",
    "    # TODO\n",
    "    # turn gender data to numerical data\n",
    "    data = np.where(data == 'F', 0, np.where(data == 'M', 10, data))\n",
    "    data = data.astype(float)\n",
    "    \n",
    "    # detect outliers\n",
    "    Q1 = np.percentile(data, 25, axis=0)\n",
    "    Q3 = np.percentile(data, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    preprocessedData = np.where((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR)), np.nan, data)\n",
    "    \n",
    "    # detect missing data and change it to mean value\n",
    "    '''col_mean = np.nanmean(preprocessedData, axis=0)\n",
    "    nan_indice = np.isnan(preprocessedData);\n",
    "    preprocessedData[nan_indice] = np.take(col_mean, np.where(nan_indice)[1])'''\n",
    "    # detect nan data and remove it\n",
    "    preprocessedData = preprocessedData[~np.isnan(preprocessedData).any(axis=1)]\n",
    "    \n",
    "    return preprocessedData\n"
   ],
   "metadata": {
    "id": "jR4TYnwwCrci",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.441162Z",
     "start_time": "2024-09-25T08:28:53.433406Z"
    }
   },
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3: Implement Regression\n",
    "You have to use Gradient Descent to finish this part"
   ],
   "metadata": {
    "id": "csS9lL8DCzZO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def Regression(dataset):\n",
    "    \"\"\"\n",
    "    Performs regression on the given dataset and return the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n",
    "\n",
    "    Returns:\n",
    "    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x + w[2] * x^2 + ...\n",
    "    \"\"\"\n",
    "\n",
    "    X = dataset[:, :1]\n",
    "    y = dataset[:, 1]\n",
    "\n",
    "    # TODO: Decide on the degree of the polynomial\n",
    "    degree = 3  # For example, quadratic regression\n",
    "\n",
    "    # Add polynomial features to X\n",
    "    # X.size() * 3\n",
    "    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, X ** d))  # Add x^d terms to feature matrix\n",
    "\n",
    "    # Initialize coefficients (weights) to zero\n",
    "    # 1 * 3\n",
    "    num_dimensions = X_poly.shape[1]  # Number of features (including intercept and polynomial terms)\n",
    "    try:\n",
    "        w = np.loadtxt(coefficient_root, delimiter=',')\n",
    "        return w\n",
    "    except:\n",
    "        w = np.zeros(num_dimensions)\n",
    "    \n",
    "    # TODO: Set hyperparameters\n",
    "    num_iteration = 1000000\n",
    "    learning_rate = 0.0000000000001\n",
    "    \n",
    "    print(w)\n",
    "    \n",
    "    # Gradient Descent\n",
    "    m = len(y)  # Number of data points\n",
    "    for iteration in range(num_iteration):\n",
    "        # TODO: Prediction using current weights and compute error\n",
    "        y_hat = np.dot(X_poly, w) # 1 * X.size()\n",
    "        error = y_hat - y\n",
    "        # TODO: Compute gradient\n",
    "        gradient = (1 / m) * np.dot(X_poly.T, error) # 1 * 3\n",
    "        # TODO: Update the weights\n",
    "        w = w - learning_rate * gradient\n",
    "        \n",
    "        # TODO: Optionally, print the cost every 100 iterations\n",
    "        if iteration % 200000 == 0:\n",
    "            cost = (1 / (2 * m)) * np.sum(np.square(error))\n",
    "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "            print(w)\n",
    "    \n",
    "    # store w vector\n",
    "    with open(coefficient_root, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for coeff in w:\n",
    "            writer.writerow([coeff])\n",
    "\n",
    "    return w\n"
   ],
   "metadata": {
    "id": "n8ftprTRC0Na",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.459268Z",
     "start_time": "2024-09-25T08:28:53.447272Z"
    }
   },
   "outputs": [],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4: Make Prediction\n",
    "Make prediction of testing dataset and store the value in *output_datalist*"
   ],
   "metadata": {
    "id": "inqQ4lh8DFY6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def MakePrediction(w, test_dataset):\n",
    "    \"\"\"\n",
    "    Predicts the output for a given test dataset using a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n",
    "                               a coefficient for the respective power of the independent variable.\n",
    "    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n",
    "                                          for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    # TODO\n",
    "    degree = 3  # For example, quadratic regression\n",
    "\n",
    "    # Add polynomial features to X\n",
    "    X_poly = np.ones((test_dataset.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, test_dataset ** d))  # Add x^d terms to feature matrix\n",
    "    \n",
    "    prediction = np.dot(X_poly, w)\n",
    "\n",
    "    return prediction\n"
   ],
   "metadata": {
    "id": "WwGE2qjgDLwt",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.487135Z",
     "start_time": "2024-09-25T08:28:53.479603Z"
    }
   },
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5: Train Model and Generate Result\n",
    "\n",
    "Use the above functions to train your model on training dataset, and predict the answer of testing dataset.\n",
    "\n",
    "Save your predicted values in `output_datalist`\n",
    "\n",
    "> Notice: **Remember to include the coefficients of your model in the report**\n",
    "\n"
   ],
   "metadata": {
    "id": "-q4qKXbDDmG9"
   }
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.514493Z",
     "start_time": "2024-09-25T08:28:53.506439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cal_MAPE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Percentage Error (MAPE) between the true and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (numpy.ndarray): The true values.\n",
    "    - y_pred (numpy.ndarray): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "    - mape (float): The MAPE value.\n",
    "    \"\"\"\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f\"Mape value: {mape:.2f}%\")\n",
    "    \n",
    "    return mape"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "id": "coo82WvZDpMq",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.550577Z",
     "start_time": "2024-09-25T08:28:53.535920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "# (1) Split data\n",
    "trainData, validationData = SplitData(training_datalist, 0.8)\n",
    "\n",
    "# (2) Preprocess data\n",
    "preprocessData = PreprocessData(trainData)\n",
    "validationData = PreprocessData(validationData)\n",
    "\n",
    "# (3) Train regression model\n",
    "coeff = Regression(preprocessData)\n",
    "\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "validation = MakePrediction(coeff, validationData[:, :1])\n",
    "mape_val = cal_MAPE(validationData[:, 1], validation)\n",
    "\n",
    "# (5) Make prediction of testing dataset and store the values in output_datalist\n",
    "output_datalist = MakePrediction(coeff, testing_datalist)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mape value: 24.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_1912\\1992744318.py:15: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  data = np.where(data == 'F', 0, np.where(data == 'M', 10, data))\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "### *Write the Output File*\n",
    "\n",
    "Write the prediction to output csv and upload the file to Kaggle\n",
    "> Format: 'Id', 'gripForce'\n"
   ],
   "metadata": {
    "id": "RW3NrFmGEEiG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Assume that output_datalist is a list (or 1d array) with length = 100\n",
    "\n",
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['Id', 'gripForce'])\n",
    "  for i in range(len(output_datalist)):\n",
    "    writer.writerow([i,output_datalist[i]])\n"
   ],
   "metadata": {
    "id": "Mo7rdhx0EFLn",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.613926Z",
     "start_time": "2024-09-25T08:28:53.605986Z"
    }
   },
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Advanced Part (45%)\n",
    "In the second part, you need to implement regression differently from the basic part to improve your grip force predictions. You must use more than two features.\n",
    "\n",
    "You can choose either matrix inversion or gradient descent for this part\n",
    "\n",
    "We have provided `lab1_advanced_training.csv` for your training\n",
    "\n",
    "> Notice: Be cautious of the \"gender\" attribute, as it is represented by \"F\"/\"M\" rather than a numerical value.\n",
    "\n",
    "Please save the prediction result in a CSV file and submit it to Kaggle"
   ],
   "metadata": {
    "id": "V1O2l8d2E3he"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "training_dataroot = 'lab1_advanced_training.csv' # Training data file file named as 'lab1_advanced_training.csv'\n",
    "testing_dataroot = 'lab1_advanced_testing.csv'   # Testing data file named as 'lab1_advanced_testing.csv'\n",
    "output_dataroot = 'lab1_advanced_OLS.csv' # Output file will be named as 'lab1_advanced.csv'\n",
    "coefficient_adv_root = 'coefficient_adv_OLS.csv'\n",
    "\n",
    "training_datalist =  [] # Training datalist, saved as numpy array\n",
    "testing_datalist =  [] # Testing datalist, saved as numpy array\n",
    "\n",
    "output_datalist =  [] # Your prediction, should be a list with 3000 elements\n",
    "degree = 1\n",
    "\n",
    "used_var = [0, 1, 2, 3, 4, 5, 6]"
   ],
   "metadata": {
    "id": "Us1kieIvcucL",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.661081Z",
     "start_time": "2024-09-25T08:28:53.654531Z"
    }
   },
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "source": [
    "# Read input csv to datalist\n",
    "with open(training_dataroot, newline='') as csvfile:\n",
    "  training_datalist = pd.read_csv(training_dataroot).to_numpy()\n",
    "\n",
    "with open(testing_dataroot, newline='') as csvfile:\n",
    "  testing_datalist = pd.read_csv(testing_dataroot).to_numpy()"
   ],
   "metadata": {
    "id": "zAmzFZM-dCkG",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.717055Z",
     "start_time": "2024-09-25T08:28:53.686507Z"
    }
   },
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implement the Regression Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.753084Z",
     "start_time": "2024-09-25T08:28:53.740512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def Regression_adv(dataset):\n",
    "    \"\"\"\n",
    "    Performs regression on the given dataset and return the coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset (numpy.ndarray): A 2D array where each row represents a data point.\n",
    "\n",
    "    Returns:\n",
    "    - w (numpy.ndarray): The coefficients of the regression model. For example, y = w[0] + w[1] * x1 + w[2] * x2 + ...\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    X = dataset[:, used_var]\n",
    "    y = dataset[:, -1]\n",
    "\n",
    "    # TODO: Decide on the degree of the polynomial\n",
    "    # Add polynomial features to X\n",
    "    # X.size() * 3\n",
    "    X_poly = np.ones((X.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, X ** d))  # Add terms to feature matrix\n",
    "\n",
    "    # Initialize coefficients (weights) to zero\n",
    "    # 1 * 3\n",
    "    num_dimensions = X_poly.shape[1]  # Number of features (including intercept and polynomial terms)\n",
    "    try:\n",
    "        w = np.loadtxt(coefficient_adv_root, delimiter=',')\n",
    "    except:\n",
    "        w = np.random.randn(num_dimensions)\n",
    "    \n",
    "    # TODO: Set hyperparameters\n",
    "    num_iteration = 500\n",
    "    learning_rate = 0.01\n",
    "    batch_size = 16  # Mini-batch size\n",
    "    decay_rate = 0.9  # RMSprop decay rate (beta)\n",
    "    epsilon = 1e-8  # Small constant to prevent division by zero\n",
    "    \n",
    "    print(w)\n",
    "    \n",
    "    # OLS\n",
    "    w = np.linalg.inv(X_poly.T @ X_poly) @ X_poly.T @ y\n",
    "    \n",
    "    # Gradient Descent\n",
    "    cache = np.zeros_like(w)\n",
    "    \n",
    "    '''m = len(y)  # Number of data points\n",
    "    for iteration in range(num_iteration):\n",
    "        # TODO: Prediction using current weights and compute error\n",
    "        y_hat = np.dot(X_poly, w) # 1 * X.size()\n",
    "        error = y_hat - y\n",
    "        # TODO: Compute gradient\n",
    "        gradient = (1 / m) * np.dot(X_poly.T, error) # 1 * 3\n",
    "        # TODO: Update the weights\n",
    "        w = w - learning_rate * gradient\n",
    "        \n",
    "        # TODO: Optionally, print the cost every 100 iterations\n",
    "        if iteration % 10000 == 0:\n",
    "            cost = (1 / (2 * m)) * np.sum(np.square(error))\n",
    "            print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "        \n",
    "        # SGD\n",
    "        shuffled_indices = np.random.permutation(len(X_poly))\n",
    "        X_shuffled = X_poly[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "    \n",
    "        for i in range(0, len(X_poly), batch_size):\n",
    "            # Mini-batch data\n",
    "            xi = X_shuffled[i:i + batch_size]\n",
    "            yi = y_shuffled[i:i + batch_size]\n",
    "            \n",
    "            # Compute gradients for the mini-batch\n",
    "            gradients = 2 / batch_size * xi.T @ (xi @ w - yi)\n",
    "            \n",
    "            # Update cache for RMSprop\n",
    "            cache = decay_rate * cache + (1 - decay_rate) * gradients**2\n",
    "            \n",
    "            # Update parameters using RMSprop\n",
    "            w = w - learning_rate * gradients / (np.sqrt(cache) + epsilon)'''\n",
    "    \n",
    "    # store w vector\n",
    "    with open(coefficient_adv_root, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for coeff in w:\n",
    "            writer.writerow([coeff])\n",
    "\n",
    "    return w"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Make Prediction"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.787657Z",
     "start_time": "2024-09-25T08:28:53.777526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def MakePrediction_adv(w, test_dataset):\n",
    "    \"\"\"\n",
    "    Predicts the output for a given test dataset using a regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - w (numpy.ndarray): The coefficients of the model, where each element corresponds to\n",
    "                               a coefficient for the respective power of the independent variable.\n",
    "    - test_dataset (numpy.ndarray): A 1D array containing the input values (independent variable)\n",
    "                                          for which predictions are to be made.\n",
    "\n",
    "    Returns:\n",
    "    - list/numpy.ndarray: A list or 1d array of predicted values corresponding to each input value in the test dataset.\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    # TODO\n",
    "    # Add polynomial features to X\n",
    "    X_poly = np.ones((test_dataset.shape[0], 1))  # Add intercept term (column of ones)\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.hstack((X_poly, test_dataset ** d))  # Add x^d terms to feature matrix\n",
    "    \n",
    "    prediction = np.dot(X_poly, w)\n",
    "\n",
    "    return prediction\n"
   ],
   "outputs": [],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO\n",
    "\n",
    "# (1) Split data\n",
    "trainData, validationData = SplitData(training_datalist, 0.8)\n",
    "\n",
    "# (2) Preprocess data\n",
    "preprocessData = PreprocessData(trainData)\n",
    "validationData = PreprocessData(validationData)\n",
    "\n",
    "# (3) Train regression model\n",
    "coeff = Regression_adv(preprocessData)\n",
    "training = MakePrediction_adv(coeff, preprocessData[:, used_var])\n",
    "mape_val = cal_MAPE(preprocessData[:, -1], training)\n",
    "\n",
    "# (4) Predict validation dataset's answer, calculate MAPE comparing to the ground truth\n",
    "validation = MakePrediction_adv(coeff, validationData[:, used_var])\n",
    "mape_val_1 = cal_MAPE(validationData[:, -1], validation)\n",
    "\n",
    "# (5) Make prediction of testing dataset and store the values in output_datalist\n",
    "testing_datalist = np.where(testing_datalist == 'F', 1, np.where(testing_datalist == 'M', 2, testing_datalist))\n",
    "testing_datalist = testing_datalist.astype(float)\n",
    "output_datalist = MakePrediction_adv(coeff, testing_datalist[:, used_var])\n",
    "\n",
    "# Save the prediction result\n",
    "with open('predictions.csv', 'a', newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([degree, mape_val, output_dataroot])"
   ],
   "metadata": {
    "id": "NSMzOXFAo2P0",
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.852342Z",
     "start_time": "2024-09-25T08:28:53.820323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-29.88171823  -0.05466904   1.34045297   0.37001345   0.03839035\n",
      "  -0.19844899   0.03351346   0.03619925]\n",
      "Mape value: 13.05%\n",
      "Mape value: 16.85%\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T08:28:53.968482Z",
     "start_time": "2024-09-25T08:28:53.955737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assume that output_datalist is a list (or 1d array) with length = 100\n",
    "\n",
    "with open(output_dataroot, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "  writer = csv.writer(csvfile)\n",
    "  writer.writerow(['Id', 'gripForce'])\n",
    "  for i in range(len(output_datalist)):\n",
    "    writer.writerow([i,output_datalist[i]])"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save the Code File\n",
    "Please save your code and submit it as an ipynb file! (**Lab1.ipynb**)"
   ],
   "metadata": {
    "id": "uVz38ASe-gGV"
   }
  }
 ]
}
